{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "operational-eligibility",
   "metadata": {},
   "source": [
    "# Map Reduce Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-switzerland",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-criminal",
   "metadata": {},
   "source": [
    "In this lesson, we'll practice working with Pyspark by looking at sales at different grocery store chains.  Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-muscle",
   "metadata": {},
   "source": [
    "### Creating our Spark Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-powell",
   "metadata": {},
   "source": [
    "Let's begin by creating our spark context.  As we know, our spark context is what connects us to our driver.  We create our context in two steps.  The first is to set up our configuration.  \n",
    "\n",
    "> Create a configuration has an application name of `groceryStores`, that connects to the local cluster on two cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "southwest-rainbow",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "statewide-pennsylvania",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"groceryStores\").setMaster(\"local[2]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-costume",
   "metadata": {},
   "source": [
    "> And from there create the Spark Context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "informational-skating",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-omaha",
   "metadata": {},
   "source": [
    "> Now if we look at the app name we can check that our app name and cluster was set up properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "artistic-halloween",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'groceryStores'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.appName\n",
    "# 'groceryStores'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "printable-sodium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[2]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master\n",
    "\n",
    "# 'local[2]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-suspect",
   "metadata": {},
   "source": [
    "### Creating our RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-fancy",
   "metadata": {},
   "source": [
    "Now to create our RDD, we'll load in some data from a pandas dataframe, and convert this dataframe to a list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adopted-inspiration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/jigsawlabs-student/pyspark-map-reduce-lab/main/supermarket_sales.csv\"\n",
    "df = pd.read_csv(url)\n",
    "sales = df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "genuine-colony",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Invoice ID': '750-67-8428',\n",
       "  'Branch': 'A',\n",
       "  'City': 'Yangon',\n",
       "  'Customer type': 'Member',\n",
       "  'Gender': 'Female',\n",
       "  'Product line': 'Health and beauty',\n",
       "  'Unit price': 74.69,\n",
       "  'Quantity': 7,\n",
       "  'Tax 5%': 26.1415,\n",
       "  'Total': 548.9715,\n",
       "  'Date': '1/5/2019',\n",
       "  'Time': '13:08',\n",
       "  'Payment': 'Ewallet',\n",
       "  'cogs': 522.83,\n",
       "  'gross margin percentage': 4.761904762,\n",
       "  'gross income': 26.1415,\n",
       "  'Rating': 9.1}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-terrorist",
   "metadata": {},
   "source": [
    "Ok, so currently, our `sales` is a list of dictionaries.  Turn this list of dictionaries into an RDD by calling the `parallelize` method and passing in the list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "secret-appendix",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_rdd = sc.parallelize(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "behavioral-zimbabwe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[11] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_rdd\n",
    "# ParallelCollectionRDD[4] at readRDDFromFile at PythonRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-species",
   "metadata": {},
   "source": [
    "> We can see that we have 1000 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "little-theta",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-footage",
   "metadata": {},
   "source": [
    "Now let's see the number of partitions for our rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "listed-cowboy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_rdd.getNumPartitions()\n",
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-vietnam",
   "metadata": {},
   "source": [
    "Ok, so now let's filter for only branch b (below you only need to use the `filter` method.  We'll call the `collect` method for you later on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "gross-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "branch_b = sales_rdd.filter(lambda row: row['Branch'] == 'B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "worst-iceland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Invoice ID': '692-92-5582',\n",
       " 'Branch': 'B',\n",
       " 'City': 'Mandalay',\n",
       " 'Customer type': 'Member',\n",
       " 'Gender': 'Female',\n",
       " 'Product line': 'Food and beverages',\n",
       " 'Unit price': 54.84,\n",
       " 'Quantity': 3,\n",
       " 'Tax 5%': 8.226,\n",
       " 'Total': 172.746,\n",
       " 'Date': '2/20/2019',\n",
       " 'Time': '13:27',\n",
       " 'Payment': 'Credit card',\n",
       " 'cogs': 164.52,\n",
       " 'gross margin percentage': 4.761904762,\n",
       " 'gross income': 8.226,\n",
       " 'Rating': 5.9}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "branch_b.collect()[0]\n",
    "# [{'Invoice ID': '692-92-5582',\n",
    "#   'Branch': 'B',\n",
    "#   'City': 'Mandalay',\n",
    "#   'Customer type': 'Member',\n",
    "#   'Gender': 'Female',\n",
    "#   'Product line': 'Food and beverages',\n",
    "#   'Unit price': 54.84,\n",
    "#   'Quantity': 3,\n",
    "#   'Tax 5%': 8.226,\n",
    "#   'Total': 172.746,\n",
    "#   'Date': '2/20/2019',\n",
    "#   'Time': '13:27',\n",
    "#   'Payment': 'Credit card',\n",
    "#   'cogs': 164.52,\n",
    "#   'gross margin percentage': 4.761904762,\n",
    "#   'gross income': 8.226,\n",
    "#   'Rating': 5.9}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-diversity",
   "metadata": {},
   "source": [
    "Ok, so from here, let's take a look at the spark ui.  We can look at the spark ui by simply typing in the spark context, and from there clicking on the link to the spark ui. \n",
    "\n",
    "> We can also directly go to http://localhost:4040, which is the designated port for spark ui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "consecutive-atlas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jeffreys-air.home:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>groceryStores</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=groceryStores>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-catering",
   "metadata": {},
   "source": [
    "When we click on the spark ui, we should see a number of jobs listed.  As we know, the job  at the top is the most recent job that was called.  Click on the most recent job and view the DAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-draft",
   "metadata": {},
   "source": [
    "<img src=\"./dag_parallel.png\" width=\"20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-advance",
   "metadata": {},
   "source": [
    "From there, click on the stage that has parallelize to dig a bit deeper into this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-airplane",
   "metadata": {},
   "source": [
    "<img src=\"./dag_collect.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-induction",
   "metadata": {},
   "source": [
    "So here it looks like Spark has first created a partitioned dataset with the RDD.  It says `readRDDFromFile`, but this is when it reads from our list of dictionaries. And then we can see that we returned a PythonRDD.  This is the list of dictionaries that was returned to us when we called `collect()`. \n",
    "\n",
    "> So above, Spark applied the filter function across the two partitions of the dataset, and then returned the results in the Python list of dictionaries in Python.  \n",
    "\n",
    "We can see that this was performed in parallel, if we scroll down and view the event timeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-channels",
   "metadata": {},
   "source": [
    "<img src=\"./green_parallel.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-waters",
   "metadata": {},
   "source": [
    "So notice, here, we see two lines, each of them represents a task run on a separate core.  Also, notice that these tasks overlab with one another, indicating that they were run in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-invalid",
   "metadata": {},
   "source": [
    "> If we look at the color coding, we can see that we see a blue for scheduling of tasks, and a longer green line for the compute time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-camping",
   "metadata": {},
   "source": [
    "### Seeing our Shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-introduction",
   "metadata": {},
   "source": [
    "Ok, now let's run a procedure that will result in shuffle.  For us, this is our group by.  We can begin with our `sales_rdd`, and from there let's group by the stores, and count the number of sales in each store."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-layer",
   "metadata": {},
   "source": [
    "> To see the results, end with a call to `collect()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "initial-church",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('C', 328), ('A', 340), ('B', 332)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_rdd.groupBy(lambda sale: sale['Branch']).map(lambda branch_sales: (branch_sales[0], len(branch_sales[1]))).collect()\n",
    "\n",
    "# [('C', 328), ('A', 340), ('B', 332)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-cover",
   "metadata": {},
   "source": [
    "After getting the correct result, take a look at the spark ui to see what occurred in the job.  For our most recent job, we should see something like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-linux",
   "metadata": {},
   "source": [
    "<img src=\"./dag_viz.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-allowance",
   "metadata": {},
   "source": [
    "And then below we see the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-commitment",
   "metadata": {},
   "source": [
    "<img src=\"./completed_stage.png\" width=\"90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-academy",
   "metadata": {},
   "source": [
    "So the first step was the `groupBy`, which occurred in parallel.  And then in the following stage partitionBy and mapPartitions occurred --  this was the count function. Let's take a closer look at the first stage -- which has the `parallelize`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-tower",
   "metadata": {},
   "source": [
    "<img src=\"./groupby_stage.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-viking",
   "metadata": {},
   "source": [
    "And on the same panel we can see the event timeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-perspective",
   "metadata": {},
   "source": [
    "<img src=\"./shuffle_event.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-honduras",
   "metadata": {},
   "source": [
    "So looking at the event timeliene, we can see that first the compute finds the records by the branch, and then, in yellow, we see that the shuffle occurs to repartition the the records by key.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-vulnerability",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-services",
   "metadata": {},
   "source": [
    "In this lab, we practiced working with Pyspark, and using the Spark UI to see the operations that occur, even when we call relatively simple methods.  We saw that by partitioning our data, methods like `filter` are performed in parallel.  And we can see that by looking at the tasks performede in a stage, and seeing that they occur simultaneously.  \n",
    "\n",
    "Then, we performed a groupby, which as we know results in shuffling.  This means that our data needs to be repartitioned, and often transferred between worker nodes to perform the operation.  We saw the shuffling occur, by taking a closer look at the stage that performed the groupby and looking at the event timeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-centre",
   "metadata": {},
   "source": [
    "<img src=\"./shuffle_event.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-study",
   "metadata": {},
   "source": [
    "### Resouorces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-matrix",
   "metadata": {},
   "source": [
    "[From Google Cloud to Pyspark](https://medium.com/@kashif.sohail/read-files-from-google-cloud-storage-bucket-using-local-pyspark-and-jupyter-notebooks-f8bd43f4b42e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
